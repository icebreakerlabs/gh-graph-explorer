#!/usr/bin/env python3
"""
Convert the Python analysis script to a Jupyter notebook
"""

import json
import os
import subprocess

def create_notebook():
    """Create a Jupyter notebook from the analysis script"""
    
    notebook = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# Cobalt Repository Social Network Analysis\n",
                    "\n",
                    "This notebook analyzes all user activity on the icebreakerlabs/cobalt repository using social network analysis to understand team collaboration patterns and engineering metrics.\n",
                    "\n",
                    "## Features:\n",
                    "- Discovers all contributors to the repository\n",
                    "- Collects GitHub activity data (issues, PRs, comments, reviews, discussions)\n",
                    "- Performs comprehensive social network analysis\n",
                    "- Visualizes collaboration patterns and team dynamics\n",
                    "- Exports data to CSV for further analysis"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "import networkx as nx\n",
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "import requests\n",
                    "import json\n",
                    "import os\n",
                    "import datetime\n",
                    "import subprocess\n",
                    "from typing import List, Dict, Any\n",
                    "from collections import defaultdict, Counter\n",
                    "import warnings\n",
                    "warnings.filterwarnings('ignore')\n",
                    "\n",
                    "# Set up plotting style\n",
                    "plt.style.use('seaborn-v0_8')\n",
                    "sns.set_palette(\"husl\")\n",
                    "plt.rcParams['figure.figsize'] = (12, 8)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Configuration\n",
                    "\n",
                    "Set your GitHub token and analysis parameters here."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Configuration\n",
                    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')\n",
                    "REPO_OWNER = 'icebreakerlabs'\n",
                    "REPO_NAME = 'cobalt'\n",
                    "\n",
                    "# Analysis period (adjustable)\n",
                    "DAYS_BACK = 30  # Change this to analyze different time periods\n",
                    "since_date = (datetime.datetime.now() - datetime.timedelta(days=DAYS_BACK)).strftime('%Y-%m-%d')\n",
                    "until_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
                    "\n",
                    "print(f\"Analyzing activity from {since_date} to {until_date}\")\n",
                    "print(f\"Repository: {REPO_OWNER}/{REPO_NAME}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 1: Discover All Contributors\n",
                    "\n",
                    "First, we'll use the GitHub API to discover all users who have contributed to the repository."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def get_repo_contributors(owner: str, repo: str, token: str) -> List[str]:\n",
                    "    \"\"\"Get all contributors to a repository\"\"\"\n",
                    "    headers = {\n",
                    "        'Authorization': f'token {token}',\n",
                    "        'Accept': 'application/vnd.github.v3+json'\n",
                    "    }\n",
                    "    \n",
                    "    url = f'https://api.github.com/repos/{owner}/{repo}/contributors'\n",
                    "    contributors = []\n",
                    "    \n",
                    "    while url:\n",
                    "        response = requests.get(url, headers=headers)\n",
                    "        if response.status_code == 200:\n",
                    "            data = response.json()\n",
                    "            contributors.extend([user['login'] for user in data])\n",
                    "            \n",
                    "            # Check for next page\n",
                    "            if 'next' in response.links:\n",
                    "                url = response.links['next']['url']\n",
                    "            else:\n",
                    "                url = None\n",
                    "        else:\n",
                    "            print(f\"Error fetching contributors: {response.status_code}\")\n",
                    "            break\n",
                    "    \n",
                    "    return contributors\n",
                    "\n",
                    "# Get contributors\n",
                    "contributors = get_repo_contributors(REPO_OWNER, REPO_NAME, GITHUB_TOKEN)\n",
                    "print(f\"Found {len(contributors)} contributors:\")\n",
                    "for contributor in contributors:\n",
                    "    print(f\"  - {contributor}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 2: Generate Repository Configuration\n",
                    "\n",
                    "Create a repos.json file with all contributors for data collection."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def create_repos_config(contributors: List[str], owner: str, repo: str) -> str:\n",
                    "    \"\"\"Create repos.json configuration for all contributors\"\"\"\n",
                    "    config = []\n",
                    "    for contributor in contributors:\n",
                    "        config.append({\n",
                    "            \"username\": contributor,\n",
                    "            \"owner\": owner,\n",
                    "            \"repo\": repo\n",
                    "        })\n",
                    "    \n",
                    "    # Get the project root directory (works in both script and notebook)\n",
                    "    try:\n",
                    "        # For script execution\n",
                    "        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
                    "    except NameError:\n",
                    "        # For notebook execution - go up from current working directory\n",
                    "        project_root = os.path.dirname(os.getcwd())\n",
                    "    \n",
                    "    config_path = os.path.join(project_root, f\"data/cobalt_{repo}_repos.json\")\n",
                    "    os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
                    "    \n",
                    "    with open(config_path, 'w') as f:\n",
                    "        json.dump(config, f, indent=2)\n",
                    "    \n",
                    "    return config_path\n",
                    "\n",
                    "# Create configuration file\n",
                    "config_path = create_repos_config(contributors, REPO_OWNER, REPO_NAME)\n",
                    "print(f\"Created configuration file: {config_path}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 3: Collect GitHub Activity Data\n",
                    "\n",
                    "Use the existing tool to collect activity data for all contributors."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def collect_github_data(config_path: str, since_date: str, until_date: str, repo_name: str) -> str:\n",
                    "    \"\"\"Collect GitHub data using the existing tool\"\"\"\n",
                    "    output_file = f\"data/cobalt_{repo_name}_activity.csv\"\n",
                    "    \n",
                    "    # Get the project root directory (works in both script and notebook)\n",
                    "    try:\n",
                    "        # For script execution\n",
                    "        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
                    "    except NameError:\n",
                    "        # For notebook execution - go up from current working directory\n",
                    "        project_root = os.path.dirname(os.getcwd())\n",
                    "    \n",
                    "    cmd = [\n",
                    "        \"uv\", \"run\", os.path.join(project_root, \"main.py\"), \"collect\",\n",
                    "        \"--repos\", os.path.join(project_root, config_path),\n",
                    "        \"--output\", \"csv\",\n",
                    "        \"--output-file\", os.path.join(project_root, output_file),\n",
                    "        \"--since-iso\", since_date,\n",
                    "        \"--until-iso\", until_date\n",
                    "    ]\n",
                    "    \n",
                    "    print(\"Collecting GitHub activity data...\")\n",
                    "    print(f\"Running command from: {project_root}\")\n",
                    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=project_root)\n",
                    "    \n",
                    "    if result.returncode == 0:\n",
                    "        print(f\"Data collected successfully: {output_file}\")\n",
                    "        return output_file\n",
                    "    else:\n",
                    "        print(f\"Error collecting data: {result.stderr}\")\n",
                    "        return None\n",
                    "\n",
                    "# Collect data\n",
                    "data_file = collect_github_data(config_path, since_date, until_date, REPO_NAME)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 4: Load and Prepare Data\n",
                    "\n",
                    "Load the collected data and prepare it for analysis."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def load_and_prepare_data(file_path: str) -> pd.DataFrame:\n",
                    "    \"\"\"Load and prepare the collected data\"\"\"\n",
                    "    # If file_path is relative, make it absolute from project root\n",
                    "    if not os.path.isabs(file_path):\n",
                    "        try:\n",
                    "            # For script execution\n",
                    "            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
                    "        except NameError:\n",
                    "            # For notebook execution - go up from current working directory\n",
                    "            project_root = os.path.dirname(os.getcwd())\n",
                    "        file_path = os.path.join(project_root, file_path)\n",
                    "    \n",
                    "    if not os.path.exists(file_path):\n",
                    "        print(f\"Data file not found: {file_path}\")\n",
                    "        return pd.DataFrame()\n",
                    "    \n",
                    "    df = pd.read_csv(file_path)\n",
                    "    \n",
                    "    # Clean and prepare data\n",
                    "    df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
                    "    df = df[df['created_at'].notnull()]\n",
                    "    df['title'] = df['title'].fillna('[No Title]')\n",
                    "    \n",
                    "    # Remove duplicates\n",
                    "    df.drop_duplicates(inplace=True)\n",
                    "    \n",
                    "    print(f\"Loaded {len(df)} activity records\")\n",
                    "    print(f\"Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n",
                    "    print(f\"Unique users: {df['source'].nunique()}\")\n",
                    "    print(f\"Activity types: {df['type'].value_counts().to_dict()}\")\n",
                    "    \n",
                    "    return df\n",
                    "\n",
                    "# Load data\n",
                    "if data_file:\n",
                    "    df = load_and_prepare_data(data_file)\n",
                    "else:\n",
                    "    print(\"No data file available. Please check the data collection step.\")\n",
                    "    df = pd.DataFrame()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 5: Build Social Network Graph\n",
                    "\n",
                    "Create a NetworkX graph from the activity data."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def build_social_network(df: pd.DataFrame) -> nx.Graph:\n",
                    "    \"\"\"Build a social network graph from activity data\"\"\"\n",
                    "    if df.empty:\n",
                    "        return nx.Graph()\n",
                    "    \n",
                    "    # Create graph from edges\n",
                    "    G = nx.from_pandas_edgelist(df, source='source', target='target', edge_attr=['type', 'created_at', 'title'])\n",
                    "    \n",
                    "    # Add node attributes\n",
                    "    for node in G.nodes():\n",
                    "        # Node activity metrics\n",
                    "        node_edges = list(G.edges(node, data=True))\n",
                    "        G.nodes[node]['activity_count'] = len(node_edges)\n",
                    "        G.nodes[node]['first_activity'] = min([edge[2]['created_at'] for edge in node_edges]) if node_edges else None\n",
                    "        G.nodes[node]['last_activity'] = max([edge[2]['created_at'] for edge in node_edges]) if node_edges else None\n",
                    "        \n",
                    "        # Activity type distribution\n",
                    "        activity_types = [edge[2]['type'] for edge in node_edges]\n",
                    "        G.nodes[node]['activity_types'] = dict(Counter(activity_types))\n",
                    "    \n",
                    "    print(f\"Built network with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
                    "    return G\n",
                    "\n",
                    "# Build network\n",
                    "if not df.empty:\n",
                    "    G = build_social_network(df)\n",
                    "else:\n",
                    "    G = nx.Graph()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 6: Calculate Social Network Analysis Metrics\n",
                    "\n",
                    "Compute comprehensive SNA metrics for all nodes."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def calculate_sna_metrics(G):\n",
                    "    \"\"\"Calculate comprehensive SNA metrics for all users\"\"\"\n",
                    "    metrics = {}\n",
                    "    \n",
                    "    # Handle eigenvector centrality for disconnected graphs\n",
                    "    try:\n",
                    "        eigenvector_cent = nx.eigenvector_centrality(G, max_iter=1000)\n",
                    "    except nx.PowerIterationFailedConvergence:\n",
                    "        eigenvector_cent = {node: 0 for node in G.nodes()}\n",
                    "    except nx.NetworkXError:\n",
                    "        eigenvector_cent = {node: 0 for node in G.nodes()}\n",
                    "    \n",
                    "    # Handle eccentricity for disconnected graphs\n",
                    "    eccentricity_dict = {}\n",
                    "    try:\n",
                    "        # Try to compute eccentricity for the whole graph\n",
                    "        eccentricity_dict = nx.eccentricity(G)\n",
                    "    except nx.NetworkXError:\n",
                    "        # If graph is disconnected, compute eccentricity for each connected component\n",
                    "        for component in nx.connected_components(G):\n",
                    "            if len(component) > 1:  # Only compute for components with multiple nodes\n",
                    "                subgraph = G.subgraph(component)\n",
                    "                try:\n",
                    "                    comp_eccentricity = nx.eccentricity(subgraph)\n",
                    "                    eccentricity_dict.update(comp_eccentricity)\n",
                    "                except nx.NetworkXError:\n",
                    "                    # If still fails, set eccentricity to 0 for these nodes\n",
                    "                    for node in component:\n",
                    "                        eccentricity_dict[node] = 0\n",
                    "            else:\n",
                    "                # Single node components have eccentricity 0\n",
                    "                for node in component:\n",
                    "                    eccentricity_dict[node] = 0\n",
                    "    \n",
                    "    for node in G.nodes():\n",
                    "        metrics[node] = {\n",
                    "            # Basic metrics\n",
                    "            'degree': G.degree(node),\n",
                    "            'activity_count': G.nodes[node]['activity_count'],\n",
                    "            \n",
                    "            # Centrality measures\n",
                    "            'degree_centrality': nx.degree_centrality(G).get(node, 0),\n",
                    "            'betweenness_centrality': nx.betweenness_centrality(G).get(node, 0),\n",
                    "            'closeness_centrality': nx.closeness_centrality(G).get(node, 0),\n",
                    "            'eigenvector_centrality': eigenvector_cent.get(node, 0),\n",
                    "            'pagerank': nx.pagerank(G).get(node, 0),\n",
                    "            \n",
                    "            # Clustering and connectivity\n",
                    "            'clustering_coefficient': nx.clustering(G, node),\n",
                    "            'local_efficiency': nx.local_efficiency(G) if G.number_of_nodes() > 1 else 0,\n",
                    "            \n",
                    "            # Network position\n",
                    "            'eccentricity': eccentricity_dict.get(node, 0),\n",
                    "            'average_neighbor_degree': nx.average_neighbor_degree(G).get(node, 0),\n",
                    "            \n",
                    "            # Activity timing\n",
                    "            'first_activity': G.nodes[node]['first_activity'],\n",
                    "            'last_activity': G.nodes[node]['last_activity'],\n",
                    "        }\n",
                    "    \n",
                    "    # Convert to DataFrame\n",
                    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
                    "    metrics_df.reset_index(inplace=True)\n",
                    "    metrics_df.rename(columns={'index': 'user'}, inplace=True)\n",
                    "    \n",
                    "    return metrics_df\n",
                    "\n",
                    "# Calculate metrics\n",
                    "if len(G.nodes()) > 0:\n",
                    "    sna_metrics = calculate_sna_metrics(G)\n",
                    "    print(f\"Calculated SNA metrics for {len(sna_metrics)} users\")\n",
                    "    \n",
                    "    # Display metrics\n",
                    "    print(\"\\nTop 10 users by degree centrality:\")\n",
                    "    print(sna_metrics.nlargest(10, 'degree_centrality')[['user', 'degree_centrality', 'betweenness_centrality', 'closeness_centrality']])\n",
                    "    \n",
                    "    print(\"\\nTop 10 users by betweenness centrality:\")\n",
                    "    print(sna_metrics.nlargest(10, 'betweenness_centrality')[['user', 'degree_centrality', 'betweenness_centrality', 'closeness_centrality']])\n",
                    "    \n",
                    "    print(\"\\nTop 10 users by closeness centrality:\")\n",
                    "    print(sna_metrics.nlargest(10, 'closeness_centrality')[['user', 'degree_centrality', 'betweenness_centrality', 'closeness_centrality']])\n",
                    "else:\n",
                    "    print(\"No nodes in the graph to analyze\")\n",
                    "    sna_metrics = pd.DataFrame()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 7: Visualize Network Metrics\n",
                    "\n",
                    "Create comprehensive visualizations of the social network analysis results."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def create_network_visualizations(G: nx.Graph, sna_metrics: pd.DataFrame, df: pd.DataFrame):\n",
                    "    \"\"\"Create comprehensive network visualizations\"\"\"\n",
                    "    if len(G.nodes()) == 0 or sna_metrics.empty:\n",
                    "        print(\"No data available for visualization\")\n",
                    "        return\n",
                    "    \n",
                    "    # Create a comprehensive visualization dashboard\n",
                    "    plt.figure(figsize=(20, 15))\n",
                    "    \n",
                    "    # 1. Network Graph\n",
                    "    plt.subplot(3, 3, 1)\n",
                    "    pos = nx.spring_layout(G, k=1, iterations=50)\n",
                    "    node_sizes = [sna_metrics[sna_metrics['user'] == node]['activity_count'].iloc[0] * 100 if node in sna_metrics['user'].values else 100 for node in G.nodes()]\n",
                    "    nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color='lightblue', \n",
                    "            font_size=8, font_weight='bold', edge_color='gray', alpha=0.7)\n",
                    "    plt.title('Social Network Graph')\n",
                    "    \n",
                    "    # 2. Degree Distribution\n",
                    "    plt.subplot(3, 3, 2)\n",
                    "    degrees = [G.degree(node) for node in G.nodes()]\n",
                    "    plt.hist(degrees, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
                    "    plt.xlabel('Degree')\n",
                    "    plt.ylabel('Frequency')\n",
                    "    plt.title('Degree Distribution')\n",
                    "    \n",
                    "    # 3. Activity Types Distribution (if available in raw data)\n",
                    "    plt.subplot(3, 3, 3)\n",
                    "    if not df.empty:\n",
                    "        activity_counts = df['type'].value_counts()\n",
                    "        plt.pie(activity_counts.values, labels=activity_counts.index, autopct='%1.1f%%')\n",
                    "        plt.title('Activity Types Distribution')\n",
                    "    else:\n",
                    "        plt.text(0.5, 0.5, 'No activity data available', ha='center', va='center')\n",
                    "        plt.title('Activity Types Distribution')\n",
                    "    \n",
                    "    # 4. Top Users by Activity Count\n",
                    "    plt.subplot(3, 3, 4)\n",
                    "    top_users = sna_metrics.nlargest(10, 'activity_count')\n",
                    "    plt.barh(top_users['user'], top_users['activity_count'])\n",
                    "    plt.xlabel('Activity Count')\n",
                    "    plt.title('Top 10 Users by Activity')\n",
                    "    \n",
                    "    # 5. Centrality Comparison\n",
                    "    plt.subplot(3, 3, 5)\n",
                    "    top_central = sna_metrics.nlargest(10, 'betweenness_centrality')\n",
                    "    plt.barh(top_central['user'], top_central['betweenness_centrality'])\n",
                    "    plt.xlabel('Betweenness Centrality')\n",
                    "    plt.title('Top 10 Users by Betweenness Centrality')\n",
                    "    \n",
                    "    # 6. Clustering Coefficient Distribution\n",
                    "    plt.subplot(3, 3, 6)\n",
                    "    clustering_coeffs = sna_metrics['clustering_coefficient'].dropna()\n",
                    "    if len(clustering_coeffs) > 0:\n",
                    "        plt.hist(clustering_coeffs, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')\n",
                    "        plt.xlabel('Clustering Coefficient')\n",
                    "        plt.ylabel('Frequency')\n",
                    "        plt.title('Clustering Coefficient Distribution')\n",
                    "    else:\n",
                    "        plt.text(0.5, 0.5, 'No clustering data available', ha='center', va='center')\n",
                    "        plt.title('Clustering Coefficient Distribution')\n",
                    "    \n",
                    "    # 7. Activity Timeline\n",
                    "    plt.subplot(3, 3, 7)\n",
                    "    if not df.empty:\n",
                    "        df['date'] = pd.to_datetime(df['created_at']).dt.date\n",
                    "        daily_activity = df.groupby('date').size()\n",
                    "        plt.plot(daily_activity.index, daily_activity.values, marker='o')\n",
                    "        plt.xlabel('Date')\n",
                    "        plt.ylabel('Activities')\n",
                    "        plt.title('Daily Activity Timeline')\n",
                    "        plt.xticks(rotation=45)\n",
                    "    else:\n",
                    "        plt.text(0.5, 0.5, 'No timeline data available', ha='center', va='center')\n",
                    "        plt.title('Daily Activity Timeline')\n",
                    "    \n",
                    "    # 8. Centrality Scatter Plot\n",
                    "    plt.subplot(3, 3, 8)\n",
                    "    plt.scatter(sna_metrics['degree_centrality'], sna_metrics['closeness_centrality'], \n",
                    "               alpha=0.6, s=sna_metrics['activity_count']*10)\n",
                    "    plt.xlabel('Degree Centrality')\n",
                    "    plt.ylabel('Closeness Centrality')\n",
                    "    \n",
                    "    # 9. Network Summary Statistics\n",
                    "    plt.subplot(3, 3, 9)\n",
                    "    plt.axis('off')\n",
                    "    stats_text = f\"\"\"\n",
                    "Network Summary:\n",
                    "Nodes: {G.number_of_nodes()}\n",
                    "Edges: {G.number_of_edges()}\n",
                    "Density: {nx.density(G):.3f}\n",
                    "Avg Clustering: {nx.average_clustering(G):.3f}\n",
                    "Avg Path Length: {nx.average_shortest_path_length(G) if nx.is_connected(G) else 'N/A'}\n",
                    "Connected Components: {nx.number_connected_components(G)}\n",
                    "    \"\"\"\n",
                    "    plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center')\n",
                    "    \n",
                    "    plt.tight_layout()\n",
                    "    plt.show()\n",
                    "\n",
                    "# Create visualizations\n",
                    "if len(G.nodes()) > 0 and not sna_metrics.empty:\n",
                    "    create_network_visualizations(G, sna_metrics, df)\n",
                    "else:\n",
                    "    print(\"No data available for visualization\")\n",
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 8: Export Data\n",
                    "\n",
                    "Export all analysis results to CSV files for further analysis."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def export_data(df: pd.DataFrame, sna_metrics: pd.DataFrame, G: nx.Graph):\n",
                    "    \"\"\"Export all data to CSV files\"\"\"\n",
                    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                    "    \n",
                    "    # Export raw activity data\n",
                    "    if not df.empty:\n",
                    "        df.to_csv(f'data/cobalt_{REPO_NAME}_raw_activity_{timestamp}.csv', index=False)\n",
                    "        print(f\"Exported raw activity data: cobalt_{REPO_NAME}_raw_activity_{timestamp}.csv\")\n",
                    "    \n",
                    "    # Export SNA metrics\n",
                    "    if not sna_metrics.empty:\n",
                    "        sna_metrics.to_csv(f'data/cobalt_{REPO_NAME}_sna_metrics_{timestamp}.csv', index=False)\n",
                    "        print(f\"Exported SNA metrics: cobalt_{REPO_NAME}_sna_metrics_{timestamp}.csv\")\n",
                    "    \n",
                    "    # Export network edges\n",
                    "    if len(G.nodes()) > 0:\n",
                    "        edges_df = pd.DataFrame(list(G.edges(data=True)), columns=['source', 'target', 'attributes'])\n",
                    "        edges_df['type'] = edges_df['attributes'].apply(lambda x: x.get('type', ''))\n",
                    "        edges_df['created_at'] = edges_df['attributes'].apply(lambda x: x.get('created_at', ''))\n",
                    "        edges_df['title'] = edges_df['attributes'].apply(lambda x: x.get('title', ''))\n",
                    "        edges_df = edges_df.drop('attributes', axis=1)\n",
                    "        edges_df.to_csv(f'data/cobalt_{REPO_NAME}_network_edges_{timestamp}.csv', index=False)\n",
                    "        print(f\"Exported network edges: cobalt_{REPO_NAME}_network_edges_{timestamp}.csv\")\n",
                    "    \n",
                    "    # Export summary report\n",
                    "    summary = {\n",
                    "        'analysis_date': datetime.datetime.now().isoformat(),\n",
                    "        'repository': f'{REPO_OWNER}/{REPO_NAME}',\n",
                    "        'analysis_period_days': DAYS_BACK,\n",
                    "        'since_date': since_date,\n",
                    "        'until_date': until_date,\n",
                    "        'total_contributors': len(contributors),\n",
                    "        'total_activities': len(df) if not df.empty else 0,\n",
                    "        'network_nodes': G.number_of_nodes() if len(G.nodes()) > 0 else 0,\n",
                    "        'network_edges': G.number_of_edges() if len(G.nodes()) > 0 else 0,\n",
                    "        'network_density': nx.density(G) if len(G.nodes()) > 0 else 0,\n",
                    "        'avg_clustering': nx.average_clustering(G) if len(G.nodes()) > 0 else 0,\n",
                    "        'connected_components': nx.number_connected_components(G) if len(G.nodes()) > 0 else 0\n",
                    "    }\n",
                    "    \n",
                    "    summary_df = pd.DataFrame([summary])\n",
                    "    summary_df.to_csv(f'data/cobalt_{REPO_NAME}_analysis_summary_{timestamp}.csv', index=False)\n",
                    "    print(f\"Exported analysis summary: cobalt_{REPO_NAME}_analysis_summary_{timestamp}.csv\")\n",
                    "\n",
                    "# Export all data\n",
                    "export_data(df, sna_metrics, G)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## Step 9: Key Insights and Recommendations\n",
                    "\n",
                    "Generate insights and recommendations based on the analysis."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "def generate_insights(sna_metrics: pd.DataFrame, G: nx.Graph, df: pd.DataFrame):\n",
                    "    \"\"\"Generate insights and recommendations from the analysis\"\"\"\n",
                    "    if sna_metrics.empty or len(G.nodes()) == 0:\n",
                    "        print(\"No data available for insights\")\n",
                    "        return\n",
                    "    \n",
                    "    print(\"=\" * 60)\n",
                    "    print(\"KEY INSIGHTS AND RECOMMENDATIONS\")\n",
                    "    print(\"=\" * 60)\n",
                    "    \n",
                    "    # Top contributors\n",
                    "    top_contributors = sna_metrics.nlargest(3, 'activity_count')\n",
                    "    print(f\"\\nðŸ† TOP CONTRIBUTORS:\")\n",
                    "    for _, user in top_contributors.iterrows():\n",
                    "        print(f\"  â€¢ {user['user']}: {user['activity_count']} activities\")\n",
                    "    \n",
                    "    # Most central users\n",
                    "    most_central = sna_metrics.nlargest(3, 'betweenness_centrality')\n",
                    "    print(f\"\\nðŸ”— MOST CENTRAL USERS (Information Brokers):\")\n",
                    "    for _, user in most_central.iterrows():\n",
                    "        print(f\"  â€¢ {user['user']}: {user['betweenness_centrality']:.3f} betweenness centrality\")\n",
                    "    \n",
                    "    # Network health indicators\n",
                    "    density = nx.density(G)\n",
                    "    avg_clustering = nx.average_clustering(G)\n",
                    "    components = nx.number_connected_components(G)\n",
                    "    \n",
                    "    print(f\"\\nðŸ“Š NETWORK HEALTH INDICATORS:\")\n",
                    "    print(f\"  â€¢ Network Density: {density:.3f} ({'High' if density > 0.1 else 'Medium' if density > 0.05 else 'Low'} collaboration)\")\n",
                    "    print(f\"  â€¢ Average Clustering: {avg_clustering:.3f} ({'Strong' if avg_clustering > 0.3 else 'Moderate' if avg_clustering > 0.1 else 'Weak'} community structure)\")\n",
                    "    print(f\"  â€¢ Connected Components: {components} ({'Good' if components == 1 else 'Fragmented'} network)\")\n",
                    "    \n",
                    "    # Activity patterns\n",
                    "    if not df.empty:\n",
                    "        activity_by_type = df['type'].value_counts()\n",
                    "        print(f\"\\nðŸ“ˆ ACTIVITY PATTERNS:\")\n",
                    "        for activity_type, count in activity_by_type.head().items():\n",
                    "            print(f\"  â€¢ {activity_type}: {count} activities\")\n",
                    "    \n",
                    "    # Recommendations\n",
                    "    print(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
                    "    \n",
                    "    if density < 0.05:\n",
                    "        print(\"  â€¢ Consider initiatives to increase cross-team collaboration\")\n",
                    "    \n",
                    "    if components > 1:\n",
                    "        print(\"  â€¢ Identify and bridge isolated team members or groups\")\n",
                    "    \n",
                    "    if avg_clustering < 0.1:\n",
                    "        print(\"  â€¢ Encourage more focused team interactions and discussions\")\n",
                    "    \n",
                    "    # Identify potential bottlenecks\n",
                    "    high_betweenness = sna_metrics[sna_metrics['betweenness_centrality'] > sna_metrics['betweenness_centrality'].quantile(0.9)]\n",
                    "    if len(high_betweenness) > 0:\n",
                    "        print(f\"  â€¢ Monitor high-centrality users for potential bottlenecks: {', '.join(high_betweenness['user'].tolist())}\")\n",
                    "    \n",
                    "    print(\"\\n\" + \"=\" * 60)\n",
                    "\n",
                    "# Generate insights\n",
                    "if not sna_metrics.empty and len(G.nodes()) > 0:\n",
                    "    generate_insights(sna_metrics, G, df)"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.5"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Write the notebook to file
    with open('cobalt-repo-analysis.ipynb', 'w') as f:
        json.dump(notebook, f, indent=2)
    
    print("Created cobalt-repo-analysis.ipynb successfully!")

if __name__ == "__main__":
    create_notebook() 